# APIéƒ¨ç½²
## ä¸€ã€ç®€å•æ‰‹å†™ç‰ˆæœ¬
### ä»£ç å‡†å¤‡
é¦–å…ˆå®‰è£…åŒ…ä¾èµ–ï¼š
```shell
pip install -U transformers fastapi accelerate
```
ç„¶åè¿è¡Œeasy_server_demo.pyï¼Œä»¥ä¸‹ä¸ºä»£ç ç¤ºä¾‹ï¼š
```
import uvicorn
import torch
from transformers import pipeline, AutoTokenizer
from fastapi import FastAPI, Request

app = FastAPI()

@app.post("/")
async def create_item(request: Request):
    global pipe
    data = await request.json()
    prompt = data.get('prompt')
    print(prompt)

    messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªè¶…çº§æ™ºè€…ï¼Œåå­—å«shareAI-llama3ï¼Œæ‹¥æœ‰ä¼˜ç§€çš„é—®é¢˜è§£ç­”èƒ½åŠ›ã€‚",
            },
            {"role": "user", "content": prompt}
    ]
    
    response = pipe(messages)
    # breakpoint()
    print(response)
    answer = {
        "response": response[-1]["content"],
        "status": 200,
    }
    return answer


if __name__ == '__main__':
    model_name_or_path = '/openbayes/home/baicai003/Llama3-Chinese-instruct-DPO-beta0___5'
    # è¿™é‡Œçš„æ¨¡å‹è·¯å¾„æ›¿æ¢ä¸ºä½ æœ¬åœ°çš„å®Œæ•´æ¨¡å‹å­˜å‚¨è·¯å¾„ ï¼ˆä¸€èˆ¬ä»huggingfaceæˆ–è€…modelscopeä¸Šä¸‹è½½åˆ°ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)
    pipe = pipeline(
        "conversational", 
        model_name_or_path, 
        torch_dtype=torch.float16, 
        device_map="auto", 
        max_new_tokens=512, 
        do_sample=True,
        top_p=0.9, 
        temperature=0.6, 
        repetition_penalty=1.1,
        eos_token_id=tokenizer.encode('<|eot_id|>')[0]
    )
    # å¦‚æœæ˜¯base+sftæ¨¡å‹éœ€è¦æ›¿æ¢<|eot_id|>ä¸º<|end_of_text|>ï¼Œå› ä¸ºllama3 baseæ¨¡å‹é‡Œæ²¡æœ‰è®­ç»ƒ<|eot_id|>è¿™ä¸ªtoken

    uvicorn.run(app, host='0.0.0.0', port=9009) # è¿™é‡Œçš„ç«¯å£æ›¿æ¢ä¸ºä½ å®é™…æƒ³è¦ç›‘å¬çš„ç«¯å£
```

ä¸Šé¢ä»£ç ä¸­ä½¿ç”¨äº†transformersçš„[pipeline](https://github.com/huggingface/transformers/blob/main/docs/source/en/conversations.md)è¿›è¡Œå®ç°ï¼Œå…·ä½“æ¥è¯´ï¼Œå®ƒç›¸å½“äºä»¥ä¸‹æ“ä½œï¼š
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# è¾“å…¥å†…å®¹
chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

# 1: åŠ è½½æ¨¡å‹ã€åˆ†è¯å™¨
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

# 2: ä½¿ç”¨å¯¹è¯æ¨¡æ¿
formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
print("Formatted chat:\n", formatted_chat)

# 3: å°†å¯¹è¯å†…å®¹è½¬ä¸ºtoken (ä¹Ÿå¯ä»¥åœ¨ä¸Šä¸€æ­¥ç›´æ¥å¼€å¯tokenize=True)
inputs = tokenizer(formatted_chat, return_tensors="pt", add_special_tokens=False)

# æŠŠtokensè½¬ç§»åˆ°GPUæˆ–è€…CPUä¸Š
inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}
print("Tokenized inputs:\n", inputs)

# 4: ä½¿ç”¨æ¨¡å‹ç”Ÿæˆä¸€æ®µæ–‡æœ¬
outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.)
print("Generated tokens:\n", outputs)

# 5: æŠŠç”Ÿæˆç»“æœä»ç¦»æ•£tokenå˜ä¸ºæ–‡æœ¬
decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)
print("Decoded output:\n", decoded_output)
```
### è°ƒç”¨æµ‹è¯•
å‘½ä»¤ï¼š
```shell
curl -X POST "http://127.0.0.1:9009"  -H 'Content-Type: application/json'  -d '{"prompt": "å…ˆæœ‰é¸¡ è¿˜æ˜¯å…ˆæœ‰è›‹"}'
```
é€šè¿‡åœ¨ç»ˆç«¯æ‰§è¡Œä»¥ä¸Šå‘½ä»¤å³å¯è°ƒç”¨ï¼Œè¿”å›ï¼š
```json
{
  "response":"ğŸ˜‚å“ˆå“ˆï¼Œè€é—®é¢˜ï¼ğŸ¤¯\n\nè¿™ä¸ªé—®é¢˜è¢«ç§°ä¸ºâ€œé¸¡å’Œè›‹çš„å¾ªç¯è®ºè¯â€ï¼Œæ˜¯æŒ‡ä¸¤ä¸ªæ¦‚å¿µç›¸äº’ä¾èµ–ã€æ— æ³•ç¡®å®šä¼˜å…ˆé¡ºåºçš„é€»è¾‘æ‚–è®ºã€‚ ğŸ“ğŸ¥š\n\nä»ç”Ÿç‰©å­¦è§’åº¦æ¥çœ‹ï¼Œé¸¡è›‹æ˜¯é¸Ÿç±»çš„ä¸€ç§ç”Ÿæ®–æ–¹å¼ï¼Œé¸¡çš„é›åŒ–è¿‡ç¨‹ä¸­éœ€è¦è›‹å­µåŒ–ï¼Œè€Œé¸¡åˆæ˜¯è›‹çš„äº§ç‰©ã€‚ ğŸ‘€\n\né‚£ä¹ˆï¼Œé—®é¢˜æ¥äº†ï¼šå¦‚æœè¯´å…ˆæœ‰è›‹ï¼Œé‚£ä¹ˆé¸¡å°±ä¸å­˜åœ¨äº†ï¼Œå› ä¸ºé¸¡æ˜¯è›‹å­µåŒ–å‡ºæ¥çš„ï¼›å¦‚æœè¯´å…ˆæœ‰é¸¡ï¼Œé‚£ä¹ˆè›‹å°±ä¸å­˜åœ¨äº†ï¼Œå› ä¸ºé¸¡æ²¡æœ‰è›‹æ¥å­µåŒ–ã€‚ ğŸ¤”\n\nè¿™ä¸ªé—®é¢˜å¯ä»¥ä»å¤šä¸ªæ–¹é¢å»ç†è§£ï¼š\n\n1ï¸âƒ£ä»æ¼”åŒ–è§’åº¦æ¥è¯´ï¼Œç”Ÿç‰©è¿›åŒ–æ˜¯ä¸€ä¸ªæ¼«é•¿çš„è¿‡ç¨‹ï¼Œé¸¡å’Œè›‹éƒ½æ˜¯è‡ªç„¶é€‰æ‹©å’Œé€‚åº”ç¯å¢ƒçš„ç»“æœã€‚ ğŸŒ³\n\n2ï¸âƒ£ä»å®šä¹‰è§’åº¦æ¥è¯´ï¼Œé¸¡å’Œè›‹éƒ½æ˜¯ç›¸äº’ä¾èµ–çš„æ¦‚å¿µï¼Œé¸¡å°±æ˜¯è›‹å­µåŒ–å‡ºæ¥çš„ï¼Œè›‹å°±æ˜¯é¸¡äº§å‡ºçš„ã€‚ ğŸ¤\n\n3ï¸âƒ£ä»å“²å­¦è§’åº¦æ¥è¯´ï¼Œè¿™ä¸ªé—®é¢˜æ¶‰åŠåˆ°æ—¶é—´æ¦‚å¿µå’Œç©ºé—´æ¦‚å¿µçš„å…³ç³»ï¼Œæ—¶é—´å’Œç©ºé—´éƒ½ä¸æ˜¯çº¿æ€§çš„ï¼Œå­˜åœ¨æŸç§ç¨‹åº¦çš„ç›¸å¯¹æ€§ã€‚ ğŸ•°ï¸\n\næ€»ä¹‹ï¼Œé¸¡å’Œè›‹çš„å…ˆåå…³ç³»åªæ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„å¾ªç¯è®ºè¯ï¼Œå®é™…ä¸Šæˆ‘ä»¬ä¸éœ€è¦æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬éƒ½æ˜¯ç”Ÿç‰©ç•Œä¸­çš„å¸¸æ€ç°è±¡ï¼ ğŸ˜Š",
  "status":200
}
```
å¦‚æœä½ éœ€è¦åœ¨å…¶ä»–å¼€å‘è¯­è¨€ä¸­ä½¿ç”¨ï¼Œå¯ä»¥ç”¨gptå°†è°ƒç”¨å‘½ä»¤è½¬æ¢ä¸ºå…¶ä»–è¯­è¨€ç‰ˆæœ¬(å¦‚pythonã€javaã€phpï¼‰

## äºŒã€ OpenAIæ ¼å¼ç‰ˆæœ¬
è¯·å‚è€ƒvLLMéƒ¨ç½²æ•™ç¨‹ï¼šhttps://github.com/CrazyBoyM/llama3-Chinese-chat/tree/main/deploy/vLLM
